{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600685530228",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Para descargar los datasets\n",
    "import download\n",
    "\n",
    "from random import shuffle\n",
    "from multiprocessing import Queue\n",
    "\n",
    "from keras.applications import VGG16\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Activation\n",
    "import sys\n",
    "\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loaded model from disk\n"
    }
   ],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"final_weight.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image specs-------------------------------\n",
    "img_size = 224\n",
    "\n",
    "img_size_touple = (img_size, img_size)\n",
    "\n",
    "# Number of channels (RGB)\n",
    "num_channels = 3\n",
    "\n",
    "# Flat frame size\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Number of classes for classification (Violence-No Violence)\n",
    "num_classes = 2\n",
    "\n",
    "# Number of files to train\n",
    "_num_files_train = 1\n",
    "\n",
    "# Number of frames per video\n",
    "_images_per_file = 20\n",
    "\n",
    "# Number of frames per training set\n",
    "_num_images_train = _num_files_train * _images_per_file\n",
    "\n",
    "# Video extension\n",
    "video_exts = \".avi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(20, 224, 224, 3)\n"
    }
   ],
   "source": [
    "def get_frames():\n",
    "\n",
    "    images = []\n",
    "\n",
    "    vidcap = cv2.VideoCapture(\"1.avi\")\n",
    "\n",
    "    success,image = vidcap.read()\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    while count<_images_per_file:\n",
    "\n",
    "        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        res = cv2.resize(RGB_img, dsize=(img_size, img_size),\n",
    "                                 interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        images.append(res)\n",
    "\n",
    "        success,image = vidcap.read()\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    resul = np.array(images)\n",
    "\n",
    "    resul = (resul / 255.).astype(np.float16)\n",
    "\n",
    "    return resul\n",
    "\n",
    "ans = get_frames()\n",
    "print(ans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = get_frames()\n",
    "\n",
    "image_model = VGG16(include_top=True, weights='imagenet')\n",
    "\n",
    "transfer_layer = image_model.get_layer('fc2')\n",
    "\n",
    "image_model_transfer = Model(inputs=image_model.input,\n",
    "                             outputs=transfer_layer.output)\n",
    "\n",
    "transfer_values_size = K.int_shape(transfer_layer.output)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4096"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "transfer_values_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfer_values():\n",
    "\n",
    "    # Pre-allocate input-batch-array for images.\n",
    "    shape = (_images_per_file,) + img_size_touple + (3,)\n",
    "\n",
    "    image_batch = np.zeros(shape=shape, dtype=np.float16)\n",
    "\n",
    "    image_batch = get_frames()\n",
    "\n",
    "    # Pre-allocate output-array for transfer-values.\n",
    "    # Note that we use 16-bit floating-points to save memory.\n",
    "    shape = (_images_per_file, transfer_values_size)\n",
    "    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n",
    "\n",
    "    transfer_values = \\\n",
    "            image_model_transfer.predict(image_batch)\n",
    "\n",
    "    return transfer_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set = get_transfer_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1, 20, 4096)"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "image_set = np.reshape(image_set, [1,20,4096])\n",
    "image_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6cc1fffc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
    }
   ],
   "source": [
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.predict(image_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.9634207 , 0.03657931]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9634207"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "score[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if score[0][0]>0.5:\n",
    "    ans = 1\n",
    "elif score[0][1]>0.5:\n",
    "    ans = 0"
   ]
  }
 ]
}