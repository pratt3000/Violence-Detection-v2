{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600842732062",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'download'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f44e3d4a1c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'download'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import download\n",
    "from random import shuffle\n",
    "from keras.applications import VGG16\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Activation\n",
    "import sys\n",
    "import h5py\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_progress(count, max_count):\n",
    "    # Percentage completion.\n",
    "    pct_complete = count / max_count\n",
    "\n",
    "    # Status-message. Note the \\r which means the line should\n",
    "    # overwrite itself.\n",
    "    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dir_fight=\"/kaggle/input/violence-final/fight\"\n",
    "dir_not_fight=\"/kaggle/input/violence-final/not_fight\"\n",
    "list_fight=os.listdir(dir_fight)\n",
    "list_no_fight=os.listdir(dir_not_fight)\n",
    "\n",
    "import random\n",
    "fight_final=random.sample(list_fight, 800)\n",
    "\n",
    "\n",
    "\n",
    "len(fight_final)\n",
    "\n",
    "\n",
    "\n",
    "no_fight_final=random.sample(list_no_fight,800)\n",
    "\n",
    "\n",
    "\n",
    "fight_labels = []\n",
    "no_fight_labels = []\n",
    "for i in range (800):\n",
    "    fight_labels.append([1,0])\n",
    "    no_fight_labels.append([0,1])\n",
    "\n",
    "\n",
    "\n",
    "final = fight_final + no_fight_final\n",
    "\n",
    "\n",
    "\n",
    "labels = fight_labels + no_fight_labels\n",
    "\n",
    "\n",
    "\n",
    "c = list(zip(final,labels))\n",
    "shuffle(c)\n",
    "    \n",
    "names, labels = zip(*c)\n",
    "\n",
    "\n",
    "\n",
    "labels[0]\n",
    "\n",
    "\n",
    "\n",
    "# Frame size  \n",
    "img_size = 224\n",
    "\n",
    "img_size_touple = (img_size, img_size)\n",
    "\n",
    "# Number of channels (RGB)\n",
    "num_channels = 3\n",
    "\n",
    "# Flat frame size\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Number of classes for classification (Violence-No Violence)\n",
    "num_classes = 2\n",
    "\n",
    "# Number of files to train\n",
    "_num_files_train = 1\n",
    "\n",
    "# Number of frames per video\n",
    "_images_per_file = 2\n",
    "\n",
    "# Number of frames per training set\n",
    "_num_images_train = _num_files_train * _images_per_file\n",
    "\n",
    "\n",
    "\n",
    "def get_frames(current_dir, file_name, c):\n",
    "    \n",
    "    in_file = os.path.join(current_dir, file_name)\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    vidcap = cv2.VideoCapture(in_file)\n",
    "\n",
    "    while(c):\n",
    "        success,image = vidcap.read()\n",
    "        c -= 1\n",
    "        \n",
    "    count = 0\n",
    "\n",
    "    while count<_images_per_file:\n",
    "                \n",
    "        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        res = cv2.resize(RGB_img, dsize=(img_size, img_size),\n",
    "                                 interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "        images.append(res)\n",
    "    \n",
    "        success,image = vidcap.read()\n",
    "    \n",
    "        count += 1\n",
    "        \n",
    "    resul = np.array(images)\n",
    "    \n",
    "    resul = (resul / 255.).astype(np.float16)\n",
    "        \n",
    "    return resul\n",
    "\n",
    "\n",
    "image_model = VGG16(include_top=True, weights='imagenet')\n",
    "\n",
    "\n",
    "input_shape = image_model.layers[0].output_shape[1:3]\n",
    "print(input_shape)\n",
    "\n",
    "\n",
    "\n",
    "# We will use the output of the layer prior to the final\n",
    "# classification-layer which is named fc2. This is a fully-connected (or dense) layer.\n",
    "transfer_layer = image_model.get_layer('fc2')\n",
    "\n",
    "image_model_transfer = Model(inputs=image_model.input,\n",
    "                             outputs=transfer_layer.output)\n",
    "\n",
    "transfer_values_size = K.int_shape(transfer_layer.output)[1]\n",
    "\n",
    "\n",
    "#print(\"The input of the VGG16 net have dimensions:\",K.int_shape(image_model.input)[1:3])\n",
    "\n",
    "#print(\"The output of the selecter layer of VGG16 net have dimensions: \", transfer_values_size)\n",
    "\n",
    "\n",
    "\n",
    "def get_transfer_values(current_dir, file_name):\n",
    "    \n",
    "    # Pre-allocate input-batch-array for images.\n",
    "    shape = (_images_per_file,) + img_size_touple + (3,)\n",
    "    \n",
    "    image_batch = np.zeros(shape=shape, dtype=np.float16)\n",
    "    \n",
    "    image_batch = get_frames(current_dir, file_name)\n",
    "      \n",
    "    # Pre-allocate output-array for transfer-values.\n",
    "    # Note that we use 16-bit floating-points to save memory.\n",
    "    shape = (_images_per_file, transfer_values_size)\n",
    "    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n",
    "\n",
    "    transfer_values =  image_model_transfer.predict(image_batch)\n",
    "    #print(transfer_values.shape)\n",
    "    return transfer_values\n",
    "    \n",
    "\n",
    "def proces_transfer(vid_names, labels):\n",
    "    \n",
    "    count = 0\n",
    "    count_i = 0\n",
    "    tam = len(vid_names)\n",
    "    image_bitches = []\n",
    "    # Pre-allocate input-batch-array for images.\n",
    "    shape = (_images_per_file,) + img_size_touple + (3,)\n",
    "    final_labels = []\n",
    "    while count<tam:\n",
    "\n",
    "        video_name = vid_names[count]\n",
    "        vidcap = cv2.VideoCapture(video_name)\n",
    "        tam_i = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if labels[count]==[0,1]:\n",
    "            in_dir= dir_not_fight\n",
    "        else:\n",
    "            in_dir= dir_fight\n",
    "        image_batches = []\n",
    "        while count_i<tam_i:\n",
    "            image_batch = np.zeros(shape=shape, dtype=np.float16)\n",
    "            image_batch = get_frames(in_dir, video_name, count_i)\n",
    "            count_i += 2\n",
    "            image_batches.append(image_batch)\n",
    "        image_bitches.append(image_batches)\n",
    "         # Note that we use 16-bit floating-points to save memory.\n",
    "        shape = (image_bitches.shape[0]*image_bitches.shape[1], image_bitches.shape[2], transfer_values_size)\n",
    "        transfer_values = np.zeros(shape=shape, dtype=np.float16)\n",
    "        \n",
    "        transfer_values =  image_model_transfer.predict(image_batch)\n",
    "         \n",
    "        labels1 = labels[count]\n",
    "        \n",
    "        aux = np.ones([image_bitches.shape[1],2])\n",
    "        \n",
    "        labelss = labels1*aux\n",
    "        final_labels.append(labelss)\n",
    "    \n",
    "        ##############################################################\n",
    "        #yield transfer_values, labelss\n",
    "        ##############################################################\n",
    "        count+=1\n",
    "    final_labels = final_labels.reshape([final_labels[0]*final_labels[1], final_labels[2]])\n",
    "    return final_labels, image_bitches\n",
    "\n",
    "\n",
    "training_set = int(len(names)*0.8)\n",
    "test_set = int(len(names)*0.2)\n",
    "\n",
    "names_training = names[0:training_set]\n",
    "names_test = names[training_set:]\n",
    "\n",
    "labels_training = labels[0:training_set]\n",
    "labels_test = labels[training_set:]\n",
    "\n",
    "frames, labels = proces_transfer(names_training, labels_training)\n",
    "frames_t, labels_t = proces_transfer(names_test, labels_test)\n",
    "frames = frames.reshape([frames.shape[1], frames.shape[0], frames.shape[2], frames.shape[3], frames.shape[4]])\n",
    "frames_t = frames_t.reshape([frames_t.shape[1], frames_t.shape[0], frames_t.shape[2], frames_t.shape[3], frames_t.shape[4]])\n",
    "\n",
    "\n",
    "chunk_size = 4096\n",
    "n_chunks = 20\n",
    "rnn_size = 512\n",
    "\n",
    "model1 = Sequential()\n",
    "input_1 = tf.keras.layers.Input(shape=(1, 4096))\n",
    "conv2d_1 = tf.keras.layers.Conv2D(32, kernel_size=3, activation=tf.keras.activations.relu)(input_1)\n",
    "pool1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)(conv2d_1)\n",
    "\n",
    "model2 = Sequential()\n",
    "input_2 = tf.keras.layers.Input(shape=(1, 4096))\n",
    "conv2d_2 = tf.keras.layers.Conv2D(32, kernel_size = 3, activation=tf.keras.activations.relu)(input_2)\n",
    "pool2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)(conv2d_2)\n",
    "\n",
    "model = keras.layers.concatenate([model1, model2])\n",
    "FC1 = Dense(1024, activation='relu')(model)\n",
    "lstm_layer = LSTM(rnn_size)(FC1)\n",
    "output = tf.keras.layers.Dense(units=2,\n",
    "                               activation=tf.keras.activations.softmax)(lstm_layer)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "full_model = tf.keras.Model(inputs = [model1, model2], outputs=[output])\n",
    "\n",
    "epoch = 200\n",
    "batchS = 500\n",
    "\n",
    "history = full_model.fit(inputs = [frames[1], frames[2]], outputs = [labels], epochs=epoch,\n",
    "                    validation_data=([frames_t[1],frames_t[2]], [labels_t]), \n",
    "                    batch_size=batchS, verbose=2)\n",
    "print(full_model.summary())\n",
    "\n",
    "result = full_model.evaluate(np.array(data_test), np.array(target_test))\n",
    "\n",
    "\n",
    "\n",
    "for name, value in zip(model.metrics_names, result):\n",
    "    print(name, value)\n",
    "\n",
    "out_dir = '/kaggle/working/'\n",
    "\n",
    "model_json = full_model.to_json()\n",
    "with open(os.path.join(out_dir,'model1.json'), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# SAVE MODEL WEIGHTS\n",
    "full_model.save_weights(os.path.join(out_dir,'model1.h5'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}